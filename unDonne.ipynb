{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<style>\n",
    "    body {font-family: Constantia;}\n",
    "    p    {font-size: 17px; text-indent: 50px}\n",
    "    pre  {font-family: Menlo;}\n",
    "</style>\n",
    "\n",
    "<!-- Modified via http://chris-said.io/2016/02/13/how-to-make-polished-jupyter-presentations-with-optional-code-visibility/ -->\n",
    "<script>\n",
    "  jQuery(document).ready(function($) {\n",
    "\n",
    "  $(window).load(function(){\n",
    "    $('#preloader').fadeOut('slow',function(){$(this).remove();});\n",
    "  });\n",
    "\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<style type=\"text/css\">\n",
    "  div#preloader { position: fixed;\n",
    "      left: 0;\n",
    "      top: 0;\n",
    "      z-index: 999;\n",
    "      width: 100%;\n",
    "      height: 100%;\n",
    "      overflow: visible;\n",
    "      background: #fff url('./assets/img/Moving%20line.gif') no-repeat center center;\n",
    "  }\n",
    "\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center id=\"top\"><h1>Donne unDonne</h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "I.    [Introduction](#Introduction)\n",
    "\n",
    "II.   [Sonnets](#Sonnets)\n",
    "\n",
    "III.  [Early Modern English](#Early-Modern-English)\n",
    "\n",
    "IV.   [Approaches](#Approaches)\n",
    "\n",
    "V.   [Description](#Description)\n",
    "\n",
    "VI.  [Discussion](#Discussion)\n",
    "\n",
    "VII. [Future Work](#Future-Work)\n",
    "\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is yet another work in progress, a final project for Dr. Noriko Tomuro's [Topics in AI: Natural Language Processing](http://condor.depaul.edu/ntomuro/courses/NLP594s18) course, CSC 594, at DePaul University: a probabilistic Petrarchan sonnet generator. Specifically, this system is a suite of tools for generating sonnets closely modeled on the work of John Donne (1572-1631), a preacher, lawyer, and poet active in the early 17c, during the reign of King James.\n",
    "\n",
    "Donne's work, reflecting his life—both imprisoned and destitute due to a marriage scandal and essentially commanded by King James to become Royal Chaplain after his conversion to Protestantism—ranged from very down-to-earth, witty love poems, to intensely emotional religious meditations. <p>\n",
    "\n",
    "Given Donne's work as input, the system will output a 14-line poem adhering to varying extent to the Petrarchan sonnet structure as introduced into English by [Sir Thomas Wyatt](https://en.wikipedia.org/wiki/Thomas_Wyatt_%28poet%29#Wyatt's_poetry_and_influence) and implemented by Donne.\n",
    "\n",
    "The system imposes sundry algorithmic constraints which encode the qualitative features described by literary tradition (i.e., scansion), and generates poetry from the parsed Donne corpus with a mixed set of possible methods, including probabilistic or stochastic context-free grammar, and smoothed and unsmoothed MLE *n*-gram modeling at the word and character levels. Vector space semantics and sentiment analysis inform these techniques alongside resources such as pronunciation and concept dictionaries and thesauri, viz. [WordNet](https://wordnet.princeton.edu/), [SentiWordNet](http://sentiwordnet.isti.cnr.it), [Lin's Thesaurus](http://www.nltk.org/_modules/nltk/corpus/reader/lin.html), and [CMUdict](http://www.speech.cs.cmu.edu/cgi-bin/cmudict)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sonnets are an ideal and somewhat traditional target for algorithmic generation due to\n",
    "their highly structured forms, in terms of features such as rhyme\n",
    "schemes (such as *ababab*), metre (e.g., iambic pentameter), and the\n",
    "number of lines. These writing constraints are useful both for\n",
    "[human](http://www.huffingtonpost.com/scott-barry-kaufman/does-creativity-require-c_b_948460.html)\n",
    "creativity—forcing thinkers to construct novel solutions—and also\n",
    "fruitfully narrow the space of generative possibilities.\n",
    "\n",
    "Thus, the foundation, in terms of NLP, begins with solving these\n",
    "formal problems: evaluating text phonetically to allow for rhyming and\n",
    "performing\n",
    "“[scansion](https://owl.english.purdue.edu/owl/resource/570/02/)” to\n",
    "ensure adherence to rhythmic requirements, with some authorial leeway \n",
    "with respect to how tightly to require rhymes or allowing for orthographic \n",
    "matching in addition to phonographic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general purpose here was to leverage the domain of English literature in a productive NLP task, establishing a foundation from which I can both learn and explore more of these techniques, and subjects, and create useful tools for writers, literary theorists, and linguists of all stripes; a sort of cross-disciplinary armanentarium. By “productive” I refer to manifesting meaningful intent as code and text, a dynamic we might diagram simply as [Intent] → [Code ↔ Text].\n",
    "\n",
    "My reasoning is that just as writing workshops and output practice are complementary to reading and listening in literature and language learning, programmatically producing poetry is a powerful means to study Digital Humanities and NLP. Doing so with tightly structured poetry allows for the exploration of algorithmic constraints alongside data-driven models.\n",
    "\n",
    "The project itself I view as something of a playground for creative, theoretical experimentation, not merely tool-making, but tool-making itself as artistic practice and literary hermeneutics.\n",
    "\n",
    "This view is perhaps influenced by my desire for a Cognitive Computational Creative Writing, with workshops acting as the labwork of Literature departments, where generating text motivates a more intensive literary analysis, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sonnets \n",
    "\n",
    "To describe Petrarchan sonnets in more detail: These 14-line poems as written in English typically adhere to a fourfold structure (4-4-4-2), in two main parts. The first eight lines (2 4-line quatrains) are called the octave or octet, which traditionally introduces a problem, and the final 6-lines form the sestet (quatrain and couplet), which might develop a solution. Each line is approximately 10 syllables, with a particular rhythm and rhyme scheme, as shown, where the octave rhyme scheme is [abba, abba] and the sestet is [cddc, ee].\n",
    "\n",
    "![](assets/img/hsdeath.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Modern English\n",
    "\n",
    "As you may have noted with the quotes from Donne’s poems in these slides, a complication here is the highly variable English spelling and pronunciation as the system was standardized over time with the spread of printing. Capitalization and typography are other components that introduce challenging variation, as described at the [OED](https://public.oed.com/blog/early-modern-english-pronunciation-and-spelling/).\n",
    "\n",
    "To list some examples, there is the silent ⟨e⟩ as seen in “Holy Sonnet X” above, interchangeable ⟨v⟩ and ⟨u⟩ depending on word position: “euer” (“ever”), interchangeable ⟨i⟩, ⟨j⟩ and ⟨y⟩ in certain cases: “ioifull praier” (“joyful prayer”), a possible capital “J” when consonant: “… to rule Justely… ”, capitalization for “important” nouns, and elocutionary punctuation, its use based on its effects on speech/sound...\n",
    "\n",
    "Additionally, Donne [disliked](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1741-4113.2008.00552.x) printing, which has added to many variations in any given poem as editors and transcribers introduced their own versions over the centuries.\n",
    "\n",
    "![](assets/img/flea.png)\n",
    "<center><i>Typographic pun from Donne’s “<a href=\"https://www.bl.uk/shakespeare/articles/a-close-reading-of-the-flea\">The Flea</a>” with long ⟨s⟩, or ſ</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches\n",
    "\n",
    "There have been many approaches to poetry generation with computers. These include using context-free grammars to generate lines, weighted with probabilities for parses induced from analyzing input text. A recent look at PCFG (Probabilistic Context-Free Grammar) for poetry generation can be seen from [Goldberg's critique](https://medium.com/@yoav.goldberg/an-adversarial-review-of-adversarial-generation-of-natural-language-409ac3378bd7) of Rajeswar, et al., [2017](https://arxiv.org/abs/1705.10929).\n",
    "\n",
    "One can also take an evolutionary approach ([Manurung, 2004](https://www.era.lib.ed.ac.uk/handle/1842/314)) and define a fitness function based on some metric (e.g., grammatical, meaningful, ‘aesthetically pleasing’) and use it to modify encoded language [genetically](https://natureofcode.com/book/chapter-9-the-evolution-of-code/) through an iterative process of selection, combination and mutation. \n",
    "\n",
    "And of course, we have various means to use the probability distributions of a text to conditionally sample from a corpus and create new sequences. These productions operate within the constraints of the type of poetic structure we want to create, such as haiku or sonnets.\n",
    "\n",
    "[Markov chains](https://shiffman.net/a2z/markov/) and the association with poetry and text generation have existed since their onset, with Andrei Markov’s modeling of Pushkin’s verse novel, *Eugene Onegin* ([1913](https://www.americanscientist.org/article/first-links-in-the-markov-chain)), or Claude Shannon's explorations, giving such results as “IN NO IST LAT WHEY CRATICT FROURE BIRS GROCID…” ([1948](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication)).\n",
    "\n",
    "For this project, I explored a probabilistic context-free grammar approach and *n*-gram likelihood-based generation at both word and character levels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description\n",
    "===========\n",
    "\n",
    "The chain of imports begins with the main file, but traces back from\n",
    "main.py to lode.py, in a somewhat convoluted sequence, so I will begin\n",
    "the description from the loading point.\n",
    "<p>\n",
    "*Components*:\n",
    "\n",
    "-   *Load (lode.py)*\n",
    "\n",
    "-   *Sentiment (sins.py)*\n",
    "\n",
    "-   *Scansion (scansion.py)*\n",
    "\n",
    "-   *Grammar (gramarye.py)*\n",
    "\n",
    "-   *POS and Word-based Generators (genesis.py)*\n",
    "\n",
    "-   *Character-level generator (karkoav.py)*\n",
    "\n",
    "- *Graphical User Interface (gui.py)*\n",
    "\n",
    "-   *Main (main.py)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from karkoav import *\n",
    "from gui import *\n",
    "\n",
    "words, words_set, sents = parse_sonnet()\n",
    "\n",
    "# Instantiate bauplan, Holy Sonnet X: 'Death be not proud... '\n",
    "hsdeath = sonnets[titles[12]]\n",
    "hslines = list(filter(lambda x: x, hsdeath.split('\\n')))\n",
    "alpha = hslines[0].split()[0]\n",
    "kpunk = end_punk_scheme(hslines)\n",
    "\n",
    "seed = hslines[0].split()[0].lower()\n",
    "cfd = nltk.ConditionalFreqDist(nltk.bigrams(tokens))\n",
    "cpd = nltk.ConditionalProbDist(cfd, nltk.LaplaceProbDist,\n",
    "                               bins=len(types))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gg = GenGUI()\n",
    "    gg.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As the background section may have suggested, choosing the data to use in this project was difficult. The simplest approach for accomplishing my goals was to balance between modernized versions to simplify parsing, with a few choice archaic versions that reflected my own preferences. In this sense, perhaps the most challenging aspect of selecting data was choosing the right blend, so to speak, to both execute the code smoothly and achieve appealing results.\n",
    "\n",
    "A small measure of preprocessing was necessary to correct what appeared to be scanning errors and which caused problems for tokenization, such as extra spaces around punctuation within words, and to mark titles to make them easier to distinguish when loading.\n",
    "\n",
    "Initially, the lode.py file loaded\n",
    "[BEEP](http://svr-www.eng.cam.ac.uk/comp.speech/Section1/Lexical/beep.html),\n",
    "a British English phonetic dictionary similar to CMU’s Pronunciation\n",
    "Dictionary, based on\n",
    "[ARPAbet](https://en.wikipedia.org/wiki/ARPABET)-style transcriptions.\n",
    "However, early versions of BEEP featured inconsistent stress markings—an\n",
    "important feature for evaluating the metre of a poem where stress is\n",
    "alternated over syllable sequences to establish rhythmic articulatory\n",
    "patterns. Later versions of BEEP simply removed the markings entirely.\n",
    "Therefore, given the primarily orthographic reception I imagine the\n",
    "project will have, I simply imported CMUdict from NLTK instead, despite\n",
    "its North American basis.\n",
    "\n",
    "At any rate, as well as importing various corpora and tokenizers, the\n",
    "loading module parses the types, tokens, lines, and files on three\n",
    "levels: a larger corpus of John Donne’s poetry and prose; just the\n",
    "sonnets and sonnet-like poems; and individual sonnets—and defines an\n",
    "initial syllable counting function. The system is essentially hardcoded\n",
    "for particular files and formats at the moment, as the guiding,\n",
    "simplifying idea behind this prototype version of the project was to\n",
    "generate sonnet structures by directly emulating a template for a given\n",
    "run, namely a particular Donne sonnet’s rhyme and metrical scheme, etc.,\n",
    "sampling from a relatively small inventory of syntactic and semantic\n",
    "elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample code from lode.py\n",
    "\n",
    "def parse_sonnet(ix: int = 12) -> list:\n",
    "    \"\"\"Collect sonnet's types, tokens, and sentences.\n",
    "       Default is HSDeath.\"\"\"\n",
    "    sonnet = sonnets[titles[ix]]\n",
    "    sents = sonnet.strip().split('\\n')\n",
    "    # \"Send me some token, that my hope may live... \"\n",
    "    tokens = [w for sent in sents\n",
    "                for w in nltk.word_tokenize(sent)\n",
    "                if w not in string.punctuation.replace(\"'\", \"\")]\n",
    "    types = set(tokens)\n",
    "    return [tokens, types, sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, the sins.py trains three models or loads them if pretrained and\n",
    "available. The *word* model is a standard Word2Vec model; the *phrase* model uses gensim’s\n",
    "phrase tools to create embeddings for trigrams (ignoring the stopwords\n",
    "in the intervening distance between content words). Primarily, this was\n",
    "intended to expand the resulting thematic clusters obtained from the\n",
    "gen\\_syn() function. This function is a modified version of available code, \n",
    "here focused on ‘subjectivity’ (1 –\n",
    "obj\\_score), with options to return synsets or orientation (positive or\n",
    "negative). There is also a function available to measure the ‘Word\n",
    "Mover’s Similarity,’ calculating inverse of the model’s\n",
    "Euclidean distance result for input of two lines, via the equation from gensim’s WmdSimilarity\n",
    "class.\n",
    "\n",
    "Also of note is the *[FastText](https://arxiv.org/abs/1607.04606v2)* model, which I discovered after searching\n",
    "for character-level vector space approaches to complement the character-level Markov\n",
    "generator, conceptually if nothing else. This function is thus set to the default, as it uses subword\n",
    "features for embeddings. It is referred to as the *k* model as *k* is\n",
    "how I have preferred to distinguish the unsmoothed character *n*-grams\n",
    "used with the order *k* Markov chain from the smoothed NLTK bigram-based\n",
    "generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample code from sins.py\n",
    "\n",
    "def phrase_model(sins: list):\n",
    "    \"\"\"Trigram-based features.\"\"\"\n",
    "    try:\n",
    "        model = Word2Vec.load('data/phrasesmod')\n",
    "    except:\n",
    "        frasier = Phrases(sins, delimiter=b' ', threshold=0, \n",
    "                          common_terms=stopwords, scoring=\"npmi\")\n",
    "        bigram_trans = Phraser(frasier)\n",
    "        trigram = Phrases(bigram_trans[sins],\n",
    "                          common_terms=stopwords,\n",
    "                          delimiter=b' ', threshold=0,\n",
    "                          scoring=\"npmi\")\n",
    "        trigram_trans = Phraser(trigram)\n",
    "        model = Word2Vec(trigram_trans[sins])\n",
    "        model.save('data/phrasesmod')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The scansion.py script defines a series of small functions for\n",
    "evaluating the parsed text according to poetic structure, namely for\n",
    "comparing lines during the generation process. This includes processing\n",
    "and filtering, counting characters, counting syllables by referencing\n",
    "the stress marks in the pronunciation dictionary’s results, evaluating\n",
    "the numbers indicating the stress, tagging lines, and comparing whether\n",
    "line ending phonetics (rhymes), metre (e.g., iambic pentameter),\n",
    "punctuation, and tag patterns match. <p>\n",
    "\n",
    "The punctuation matching is part of\n",
    "the aforementioned emulation, which will be discussed shortly. An\n",
    "additional function analyses the “lexical diversity,” or type-token\n",
    "ratio, as this has been referenced as a useful metric for establishing\n",
    "the aesthetic effects of poetry by\n",
    "[Simonton](https://link.springer.com/article/10.1007/BF00123412), et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample code from scansion.py\n",
    "\n",
    "def get_ttr(lines: list) -> float:\n",
    "    \"\"\"Return type-token ratio.\"\"\"\n",
    "    lines = '\\n'.join(lines)\n",
    "    tokens = filter_lline(nltk.word_tokenize(lines))\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    types = set(stems)\n",
    "    return len(types)/len(tokens)\n",
    "\n",
    "def mimick(line: str, punk: str) -> str:\n",
    "    \"\"\"Capitalize line and add required punctuation.\"\"\"\n",
    "    return line[:1].upper() + line[1:] + punk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The gramarye.py file encompasses my crude attempts at using an\n",
    "aesthetically defined grammar that I extrapolated from parts of the\n",
    "sonnets that I prefer, to induce a probabilistic context-free grammar\n",
    "for generating new lines that mimic that grammar. A function for\n",
    "defining said grammar writes the rules for the regular expression\n",
    "parser, which parses a sentence and uses regular expressions to rewrite\n",
    "it into Penn Treebank format, and creates a Tree from the resulting\n",
    "string, doing so repeatedly to create a treebank of sorts. \n",
    "\n",
    "The other functions in the file are modifications of NLTK’s CFG and ProbDist\n",
    "generate functions, adding an analysis of probabilities to the tree\n",
    "traversal and terminal selections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/img/pcfg.png)\n",
    "<p>\n",
    "<center><small><em>[PCFG example tree via Hoffman, 2009 slide, in turn via Jurafsky & Schutze](https://courses.cs.washington.edu/courses/cse590a/09wi/pcfg.pdf)</em></small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending from this, the genesis.py module defines a function to parse\n",
    "the sonnets’ lines per the above grammar functions, by inducing a grammar\n",
    "from the custom treebank and invoking the modified generate methods. As\n",
    "this can be a slow process, a timer allows the process to expire after an\n",
    "adjustable limit. Two functions are associated with the word-level\n",
    "generator using the Laplace-smoothed conditional frequency distribution\n",
    "of bigrams. This basic smoothing was used primarily to establish foundational working code for the project.\n",
    "\n",
    "The first function, *gen\\_from\\_cfd*, initiates the process within broad\n",
    "constraints to create a sonnet-like structure, and pre- and post-\n",
    "process the results from the *generate\\_model* function, which generates\n",
    "words and evaluates potential results before appending them by using the\n",
    "scansion functions to compare with the ‘ur’ sonnet, the template, which\n",
    "by default is Holy Sonnet X, “Death be not proud… ” At the moment, this\n",
    "focuses on requiring 10 syllables, rhyming, and a measure of\n",
    "subjectivity, sentiment-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample code from genesis.py\n",
    "\n",
    "def gensen(st: list, n: int = 20, d: int = 4, m: int = 13) -> list:\n",
    "    \"\"\"Induce stochastic CFG from sonnet 'treebank'.\n",
    "       Use modified NLTK functions to probabilistically generate new sentences.\"\"\"\n",
    "    gensents = []\n",
    "    sontag = [gram(sent) for sent in st]\n",
    "    prodsp = [p for _, t in sontag\n",
    "                       for p in t.productions()]\n",
    "    grammarp = induce_pcfg(Nonterminal(\"S\"), prodsp)\n",
    "    for sentence in generated(grammarp, n=n, depth=d):\n",
    "        if len(sentence) <= m:\n",
    "            gensents.append(' '.join(sentence))\n",
    "    return gensents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have karkoav.py, which defines a class and its methods for the\n",
    "character-level generator. This was a late addition to the project that\n",
    "became extensive, scratch-built Markov chain code with, ironically, the\n",
    "most promise in meeting the project goals. This was inspired by Yoav\n",
    "Goldberg’s demonstration in response to Andrej Karpathy’s famous\n",
    "char-RNN\n",
    "[article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/),\n",
    "Goldberg showing that a char-OMM (to use Jurafsky, et al.’s acronym for\n",
    "Markov chains, “Observable Markov Models”) of suitable order *k* is very\n",
    "effective, without the compute costs of the char-RNN.\n",
    "\n",
    "The constructor first uses a sequence of functions to train or reload a\n",
    "dictionary containing *k*-gram state keys and their transition\n",
    "probabilities based on potential next-characters. When *.generate()* is\n",
    "called in the main file, it operates line-by-line, calling functions\n",
    "which select *k*-grams to act as seeds and sampling subsequent\n",
    "characters under a rather complicated series of conditions: essentially\n",
    "attempting to select seeds from thematic clusters found by the FastText\n",
    "and other models based on their similarity to previous seeds for\n",
    "cohesion, falling back on more random seeds, and constraining and\n",
    "mimicking the sonnet form similar to the other generators based on\n",
    "conditions of length and the metrical results, etc. from the scansion\n",
    "functions, and post-processing lines with a *\\_mimickry* function to\n",
    "emulate the template punctuation.\n",
    "\n",
    "The results have a title appended based on similar seed logic, searching\n",
    "for synonyms for the title of the sonnet which contributed the most to\n",
    "the generated sonnet, where synonym results are available, falling back\n",
    "on randomly selected sonnet words’ synonyms.\n",
    "\n",
    "Most recently, I added a simple visualization, as described in the future work section below.\n",
    "\n",
    "Finally, the main.py file calls functions to load and define the corpora\n",
    "and template sonnet, and accepts command-line arguments to choose\n",
    "between the above generators, printing the respective results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample code from karkoav.py\n",
    "\n",
    "def _train_mod(self) -> dict:\n",
    "    \"\"\"Create dictionary of letter k-grams;\n",
    "       Values are tallies of letters that follow k chars.\n",
    "    \"\"\"\n",
    "    k = self._k\n",
    "    model = defaultdict(Counter)\n",
    "    for line in self._imp:\n",
    "        line = ''.join(filter(lambda x: x in string.printable, line))\n",
    "        # Start state; preserving final EOL observations to shift probabilities, \n",
    "        # allow for constraints, such as rhyme check.\n",
    "        lex = [\"<s>\"] + list(line)\n",
    "        for i in range(len(lex)-k): # Update count for distribution.\n",
    "            model[tuple(lex[i:i+k])][lex[i+k]] += 1\n",
    "    self._prob_dist(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, the main.py file calls functions to load and define the corpora\n",
    "and template sonnet, and accepts command-line arguments to choose\n",
    "between the above generators, printing the respective results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion\n",
    "==========\n",
    "\n",
    "Analysis\n",
    "--------\n",
    "The design of constraints in combination with the generators posed a far\n",
    "more difficult problem than expected. Using\n",
    "the basic bigram probability distributions as a foundation, I was\n",
    "able to incorporate rough phonetic information from CMU's pronunciation dictionary through the\n",
    "use of the scansion functions described above. Though formally correct\n",
    "for the most part, there was something lacking in the results. \n",
    "\n",
    "Here is an example:\n",
    "\n",
    "> Of rusty iron ground teach My indisposed parts hee\n",
    "> \n",
    "> This even so But alas is true no;\n",
    "> \n",
    "> Than nature Which hell or think that borrow\n",
    "> \n",
    "> Love clergy only Who abroad I we.\n",
    "> \n",
    "> Yong Contemptuous Yet to thee first me,\n",
    "> \n",
    "> Are aptest to stand stiff till doom be profane lo,\n",
    "> \n",
    "> Candles Which Now receive such gay goe doe,\n",
    "> \n",
    "> Love Philosophy But might try freely.\n",
    "> \n",
    "> Spoken well a calfe an arm and trifle When,\n",
    "> \n",
    "> Which drives them upon Thy Prophets and well;\n",
    "> \n",
    "> Hat to sleepe or make that sacrifice Which hell,\n",
    "> \n",
    "> And flexible to pursue things we then?\n",
    "> \n",
    "> All ages were in ranke itchie lust and country,\n",
    "> \n",
    "> If they spend more Holy mourning as I.\n",
    "\n",
    "The rhymes match the source sonnet, as well as the punctuation, as\n",
    "explicitly conditioned and appended. The syllable counts are mostly in\n",
    "the range of 10 per line, although the mixture of archaic language and\n",
    "the relatively small amount of data in the CMU dictionary made accuracy\n",
    "difficult. This became a frequent theme: performing automatic analysis\n",
    "on words such as “slipperinesse,” written in the older style, variant\n",
    "spellings cause errors and a kind of lossiness, missed opportunities.\n",
    "\n",
    "Likewise we can see inconsistent capitalization, although in this case I\n",
    "believe I failed to properly implement line capitalization.\n",
    "Capitalization is inconsistent and somewhat arbitrary in early Englishes, and I did attempt to\n",
    "honor these authorial choices when tokenizing the text, although this\n",
    "can be refined with domain knowledge (recognize the purpose of emphasis,\n",
    "common noun-orientation), to counterbalance potential editorial and\n",
    "compositor errors and influences.\n",
    "\n",
    "I believe the main fault is the sense of morphological rigidity and a\n",
    "general syntactic incoherence with abrupt shifts. This could be\n",
    "mitigated with more careful algorithm design, and/or blending with other\n",
    "models with subword information, as we’ll see.\n",
    "\n",
    "From here, I wanted to take a more syntactic approach, but found the\n",
    "basic CFG a bit too straightforward. Thus, I went with a PCFG. The\n",
    "problem here was inducing a grammar from the tree productions, without a\n",
    "pertinent treebank. Perhaps I misunderstood the pipeline, but I had the\n",
    "impression that I needed to create my own. I took this opportunity to\n",
    "apply a method I came up with years ago for “reverse canoneering”\n",
    "(reverse engineering canonized works) structured poems as a creative\n",
    "practice: essentially applying a series of arbitrary, stylized\n",
    "algorithmic transformations to the structure of a poem to extrapolate a\n",
    "template, a form of custom, generative pareidolia, we might say. Such as deciding to look at the blend of voiced and plosive\n",
    "sounds, the shape of lines, the inflections and parts-of-speech,\n",
    "metaphors (containers, etc.), and the like. This notion was originally an exploration of bottom-up techniques to create personalized literary canons, loosening traditional interpretive constraints regarding form especially.\n",
    "\n",
    "In this case, now aided with a computer, I pored over the sonnets and\n",
    "focused on the tag patterns from segments of lines, creating a custom\n",
    "grammar for it. It took time to create rules that wouldn’t recursively,\n",
    "endlessly expand, or divide the poems too finely, and what I have now is\n",
    "a bit more baroque than I would like and generally reflects practical more than\n",
    "aesthetic goals, but at least it’s far more efficient than earlier versions. It\n",
    "also doesn’t seem to create very good results, although it sometimes\n",
    "produces real gems; it’s hit or miss:\n",
    "\n",
    "> Thirst art read from woe flesh\n",
    "> \n",
    "> O his envious soul with me us can kill;\n",
    "> \n",
    "> For all art now\n",
    "> \n",
    "> All this and as than my deign.\n",
    "> \n",
    "> Infinities gained already,\n",
    "> \n",
    "> Jacob profane nor thou on wrath still only,\n",
    "> \n",
    "> Sleep made for all which at dost minerals,\n",
    "> \n",
    "> Nor fills not and holds long and numberless.\n",
    "> \n",
    "> Endless business me made in whose,\n",
    "> \n",
    "> Forgiveness glorified from ends thine end mild weakness;\n",
    "\n",
    "This is from a shorter, simpler grammar than I have now. I tried out\n",
    "countless grammars, and suspect I am missing a key piece of logic to\n",
    "make the system work properly. Perhaps a combination of something like\n",
    "the much larger Penn Treebank extended with Donne’s sentences, or\n",
    "perhaps simply parsing Donne’s sentences via Stanford would be best, a process that will require some adjustment to current tag patterns.\n",
    "\n",
    "Again the problem with archaic language came about, with an\n",
    "overabundance of nouns in the results. I imagine this made gleaning\n",
    "information from entropic differences difficult.\n",
    "\n",
    "From the current, larger grammar, before timing out:\n",
    "\n",
    "![](assets/img/pcfgen.png)\n",
    "\n",
    "Finally, I applied the character-level approach, which consumed the bulk\n",
    "of my experimentation with various constraints and combinations. This\n",
    "approach proved to produce quite coherent results, presumably because of\n",
    "the flexibility introduced by the subword-based procedure. However, I\n",
    "had difficulties constraining this flexible generation with the same\n",
    "rules applied to the other generators. Instead, I tried to focus on more\n",
    "semantic and affective aspects, before eventually applying a rather clumsy rhyme constraint that essentially collects rhymes from the source sonnets and appends them to generated lines.\n",
    "\n",
    "Using Word2Vec’s similar embeddings gave somewhat choppy results, hence\n",
    "I made use of multiword embeddings, before discovering that I could also\n",
    "use FastText, which seemed appropriate in its use of subword features. I\n",
    "did not quantitatively compare the results, but the sets were subtly\n",
    "different and I felt the latter model added coherence. Since we might\n",
    "describe such results as thematic (vs. semantic), I felt it plausible\n",
    "that I might introduce and maintain themes over the poem by integrating\n",
    "these as seeds in the *k*-gram:char dictionary when possible.\n",
    "\n",
    "Around this point, I began trying to introduce more properties of\n",
    "Donne’s style into the sonnets. For instance, his Holy Sonnets are known\n",
    "for using the Petrarchan tradition to introduce a sort of spiritual\n",
    "problem in the first 8 lines (two quatrains)—the octet or octave, with a\n",
    "turn at the 8th line (the “volta”) toward resolving the problem in a\n",
    "more meditative tone over the final 6 lines (the final 4-line quatrain\n",
    "and the couplet). I decided I might influence the generated poems by\n",
    "requiring that the octet have a more subjective sentiment, allowing\n",
    "it to wane into more neutral tones in the final lines. This actually did\n",
    "seem to have some effect, though perhaps it was my imagination playing\n",
    "tricks, in the same way that generating titles seemed to influence my\n",
    "reading of the poems. \n",
    "\n",
    "Here is an example of a result for the char-OMM via the work-in-progress GUI:\n",
    "\n",
    "![](assets/img/jacob.png)\n",
    "\n",
    "At one point when evaluating results I realized I’d\n",
    "made an indexing error in my code by reading the poems and sensing a\n",
    "“turn” around the 7th line where I’d intended it to be at the 8th\n",
    "(we often see uses of words like “alas” or mid-line full stops\n",
    "accompying this in results), so I suppose that basic bit of sentiment\n",
    "manipulation had some effect, though I had the sense it took a line or\n",
    "two for the probabilities to shift accordingly.\n",
    "\n",
    "As this model took a line-based approach—as opposed to the full circular\n",
    "scans over the input used by Goldberg in his response to Karpathy, we\n",
    "preserve these line-specific lexical distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further discussion\n",
    "\n",
    "For the PCFG, despite customizable tree depth limits, it took time to write clauses that wouldn’t cause the induction function to spin endlessly, and ultimately the archaic language, I suspect, is difficult to tag. The parser often sees long strings of nouns or verbs rather than potential predicate-argument structures and the like. Still, it has potential as a constraint for the other models.\n",
    "<p>\n",
    "For instance, an early result can be seen here:\n",
    "> \n",
    "<div style=\"font-family: Garamond; text-indent: 0px;\">All dim flower pilgrim shall thee arise<br>\n",
    "Heaven doth my everlasting profane;<br>\n",
    "Unto dissemble wills's sign and jointure<br>\n",
    "Him yet who hath and slave death.<br>\n",
    "It at my st's dwell now wilt,<br>\n",
    "Of I invest sinned and imprisoned and so i,<br>\n",
    "Son loath flesh time and let he died,</div>\n",
    "\n",
    "The Penn POS tags for this were:<br>\n",
    "> \n",
    "<div style=\"font-family: Garamond; text-indent: 0px;\">All/DT dim/NN flower/NN pilgrim/VBP shall/MD thee/VB arise/NN<br>\n",
    "Heaven/NNP doth/CC my/PRP\\$ everlasting/NN profane/NN;<br>\n",
    "Unto/NNP dissemble/JJ wills’s/NN sign/NN and jointure/NN<br>\n",
    "Him/NNP yet/RB who/WP hath/NN and/CC slave/VB death/NN.<br>\n",
    "It/PRP at/IN my/PRP$ st’s/NN dwell/NN now/RB wilt/VBP,<br>\n",
    "Of/IN I/PRP invest/VBP sinned/VBN and/CC imprisoned/VBN and/CC so/RB i/JJ,<br>\n",
    "Son/NNP loath/NN flesh/JJ time/NN and/CC let/VB he/PP died/VB,</div>\n",
    "\n",
    "The bigram model was relatively easy to implement and constrain, because we are working with conventional tokens, but the whole-word approach to constructions gave a feeling of morphological stiffness with respect to stems and affixes. When working on combinations at this level, relying on present and previous word conditions, the range of possibilities feels too limited, with a narrow permutation space of length ~8 vs. ~40.\n",
    "\n",
    "Best results were achieved, subjectively speaking, with the character-level approach. With a larger feature space, we can map a single author’s style more effectively, I think, with the small, signature details. We can also handle morphology with respect to lexical stems and functional affixes, giving more fluidity to the results. It was very interesting to watch the model produce “endless infinities” or “endless in me” from occurrences of “numberless infinities” and “endless.” \n",
    "\n",
    "We can also manipulate results at the phonemic level. At the same time, I found the lines harder to weave into a consistent poetic form. There is a poor correspondence to dictionary headwords, and the results shift rapidly from verbatim mimicry to gibberish with a change in order from, say, 4 to 5 characters. Yoav Goldberg, in his response to Andrej Karpathy that inspired the use of this model, noted a lack of context awareness for such models, as opposed to an RNN learning about opening and closing brackets, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This proved to be more challenging that I intended, and many of the problems began with choosing an archaic mode of English to constrain closely, rather than allowing automatic, wide-ranging inference from machine learning procedures. The creative process starts at the choice of data. Any decision affects both practical and artistic acceptability.\n",
    "<p>\n",
    "I consider this project a failure in terms of implementing available tools well, and producing something worthwhile. But a success as a learning experience. Building the Markov chain from scratch and attempting to induce a useful grammar gave me a much deeper understanding, complementing my knowledge of neural networks.\n",
    "<p>\n",
    "The most fundamental realization I had, practically speaking, was that for acceptable results as a tool or cultural artefact, I need to combine approaches, and ensure the system can be tweaked and tuned by users at every step, at simple or more advanced levels.\n",
    "<p>\n",
    "Some interesting observations I had was that adding a title to the generated sonnets influenced my perception, as I began searching for a relationship when reading, and also how many latent variables, so to speak, can be captured by low-level models, such as revealing frequent patterns like alliteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "<p>\n",
    "One can see the value of production-oriented projects for learning, as it forces us to [notice](https://www.victoria.ac.nz/lals/about/staff/publications/paul-nation/2007-Four-strands.pdf#page=3) [PDF, pp. 3-4] gaps in our more receptive abilities that we’ve glossed over when reading and listening, or when writing code where data analysis and interpretation is the final step. \n",
    "<p>\n",
    "In this way, the role of NLG in Computer Science curricula might be analagous to how I envision Creative Writing workshops in Literature departments as STEM-like labs, complementing the receptive with the productive to learn and master concepts.\n",
    "\n",
    "In interdisciplinary terms, there are rich, untapped resources in theory and criticism for NLG, as one might find when exploring analyses of structured poetry. In turn, NLG can reveal more in literature and augment artistic production.\n",
    "<p>\n",
    "Workshops might create a feedback loop, with computation quantifying and cognitive science analyzing writing to augment and attenuate it, and reciprocally, creativity and theory force may awareness of subjectivity and plasticity in these empirical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future Work\n",
    "-----------\n",
    "\n",
    "The code is in dire need of\n",
    "refactoring, and especially symbolic\n",
    "restructuring may be needed, with constraint handling searches and\n",
    "structures as seen in Peter Norvig and Stuart Russell’s seminal *AIMA*\n",
    "textbooks.\n",
    "\n",
    "We might also blend this model with the PCFG\n",
    "model, or at the least, parts-of-speech tags, by creating Markov chains\n",
    "based on template sonnet tag schemes, comparing sequences of results to\n",
    "ensure syntactic validity, and/or by using an evolutionary algorithm to\n",
    "blend and mutate results from different models, per the work of\n",
    "Manurung, et al. ([2004](https://www.era.lib.ed.ac.uk/handle/1842/314)).\n",
    "\n",
    "More fine-grained use of sentiment is also of interest, especially in\n",
    "mirroring source texts to imitate the personality of Donne to create a\n",
    "kind of “flatline construct”—Donne frequently uses a rough humor in his language, in contrast to the\n",
    "loftier spiritual tones. Rhyming is a difficult component in part\n",
    "because the transmission is textual primarily, and read by readers who\n",
    "speak and subvocalize many Englishes.\n",
    "\n",
    "British English itself no longer resembles the earlier versions, where\n",
    "words such as “prove” once rhymed with “love,” as David [Crystal](https://www.youtube.com/watch?v=YGO7TYQs4dY) and his\n",
    "son often point out with Shakespeare and what’s called OP: “Original\n",
    "Pronunciation”. Thus, I did not\n",
    "prioritize too strongly that implementation of rhymes at this stage. But I do believe that\n",
    "more complicated algorithms, looking both\n",
    "forward and backwards, might be useful in this regard, allowing for\n",
    "smoother backing off and re-attempting constructions. I have recently found work\n",
    "related to this from Reddy & Knight\n",
    "([2011](http://www.aclweb.org/anthology/P11-2014)).\n",
    "\n",
    "At present, the Tkinter GUI is still in development. I had initially wanted an interactive interface for the program via\n",
    "Tkinter or web-based GUI, and intend at least to allow for user input\n",
    "which could weight and shift results, making it a more useful writing\n",
    "tool. This would also impel me to include piecewise visualization of the\n",
    "generation process, in a way that would not add too much overhead.\n",
    "Perhaps keeping logs for later reconstruction, rather than on-line.\n",
    "\n",
    "More careful proofreading and preprocessing of source texts is\n",
    "necessary, I think, and this brings us to the tricky aspect of the many,\n",
    "many versions of Donne’s poetry. There is a massive\n",
    "[tome](http://donnevariorum.tamu.edu/) analyzing these variations due to\n",
    "editorial choices and mistakes over the centuries since his work was\n",
    "published. Which versions to use as “legitimate” ground truth is\n",
    "difficult; I chose versions here that I considered modern enough to have\n",
    "many dictionary results and consistent matches (the more archaic\n",
    "versions vary more within texts, I believe), offset by some more archaic\n",
    "versions that I personally found appealing, to give the results a more\n",
    "aesthetically appealing flavor.\n",
    "\n",
    "This dynamic is of interest, also: when designing these as writing\n",
    "tools, how much to hardcode one’s personal preferences, and/or to design\n",
    "the code so that it can be attenuated or generalized, adapted. We might\n",
    "attempt to reproduce strictly the nuances of Donne’s work, or create a\n",
    "general sonnet generator, or make the code itself more a part of the\n",
    "poems, so that it forms a seamless creative artefact from\n",
    "author/programmer to the produced text, which may or may not be a\n",
    "protean, endlessly interactive work, or something more stationary and\n",
    "immutable.\n",
    "\n",
    "A visualization I have in mind is one\n",
    "inspired from encountering Ron Padgett's *Creative Reading* a couple of\n",
    "years ago:\n",
    "\n",
    "![](assets/img/padgett.png)\n",
    "\n",
    "That is, I wanted to trace the path of how the language model “read” the\n",
    "inputs to produce the sonnet, by looking at the most contributing\n",
    "sonnets per line and indexing initial parallels, color-coding lines and\n",
    "tracing as we move across the output’s lines to where they would have\n",
    "been in the input. I began a function for this with some tools in mind,\n",
    "which spawned the function I am currently using for title generation.\n",
    "\n",
    "Currently, the visualization is in the proof-of-concept stages, and rather weak proof, at that.\n",
    "\n",
    "![](assets/img/jacobvis.png)\n",
    "\n",
    "I considered analyzing the phonetic inventory of the result poems to\n",
    "influence linearity and curvature to reproduce the “[bouba/kiki\n",
    "effect](https://en.wikipedia.org/wiki/Bouba/kiki_effect),” the tendency\n",
    "to assign “sharp” or “round” visuals to consonants like “k” and “b,”\n",
    "respectively.\n",
    "\n",
    "For evolutionary and other purposes, I also need to devise a more\n",
    "comprehensive metric for the fitness function, which again would be\n",
    "influenced by user input. This touches a bit on using as the “standard”\n",
    "the source, or the user’s perceptions, reflecting perhaps the conflict\n",
    "in literary theory over how closely to incorporate the author’s intent\n",
    "into the reading of a work.\n",
    "\n",
    "When I reviewed capitals in results, I found myself uncertain whether\n",
    "Donne had intended the capitals mid-line, for various reasons, or\n",
    "whether bugs in my program had caused this. I realized that the age-old question—for those who choose to ask, went from “What did the author mean?” to “What properties in the input and code caused the mechanistic production of this result?”, with factors to consider including the authors’ intentions and mistakes, with ‘authors’ ranging from Donne to editors and compositors of varying time periods (e.g., Early Modern English in the early 17th century and its capitalization practices) to the programmers involved with various modules used, computer scientists’ design of algorithms that were implemented by the programmers, data used in the creation of various models used for parsing and sentiment analysis, and of course myself. \n",
    "\n",
    "The process of production and comprehension was intimately intertwined\n",
    "here, and I found that having some measure of domain-knowledge made this\n",
    "process easier, allowing me to compose code with respect to the poetry’s\n",
    "properties from memory, rather than having to continually consult the\n",
    "source text(s). Typically, it is knowledge of code and maths that would\n",
    "streamline that composition process. To me, this emphasized the\n",
    "uniqueness of systems such as this that cross domains, and the\n",
    "increasing importance of practitioner diversity and participation for\n",
    "the ever-widening gyre of code’s integration into society, to ease\n",
    "cognitive load and/or quickly catch mistakes during development, for\n",
    "example."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<footer id=\"attribution\" style=\"float:right; color:#999; background:#fff;\">\n",
    "<a href=\"https://erikmcguire.github.io/\">© Erik McGuire</a>, 2018\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
